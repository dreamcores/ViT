{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc92c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 128, 42])\n",
      "ViT-3 out: torch.Size([1, 128, 42])\n",
      "flatten-3 out: torch.Size([1, 5376])\n",
      "{tensor([[-0.0758]], grad_fn=<AddmmBackward0>), tensor([[-0.1046]], grad_fn=<AddmmBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    # input: (1, 4, 31, 10, 3)\n",
    "    # output: (1, 128, 42)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.Conv3D_block = nn.Conv3d(in_channels=4, out_channels=128, \n",
    "                                      kernel_size=(5, 5, 3), # (D,H,W)\n",
    "                                      stride=(4,4,1),\n",
    "                                      padding=(0,0,1))\n",
    "        \n",
    "        self.pos_emb = nn.Parameter(torch.randn(1, 128, 42))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Conv3D_block(x)\n",
    "        x = x.flatten(2) \n",
    "        x += self.pos_emb\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        assert d_model % num_heads ==0, \"dim must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.dk = d_model // num_heads\n",
    "        self.scale = self.dk ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.dk).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x        \n",
    "\n",
    "\n",
    "class Attentionblock2(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=6, dropout=0.1):\n",
    "        super(Attentionblock2, self).__init__()\n",
    "        assert d_model % num_heads ==0, \"dim must be divisible by num_heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.dk = d_model // num_heads\n",
    "        self.scale = self.dk ** -0.5\n",
    "\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "        #输出层\n",
    "        self.proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        B, N, C = x.shape # batch, tokens, d_model\n",
    "\n",
    "\n",
    "        #(B, N, C)→(B, N, H, dk)→(B, H, N, dk),每个head内部的计算是并行的\n",
    "        Q = self.Wq(x).reshape(B, N, self.num_heads, self.dk).permute(0, 2, 1, 3)\n",
    "        K = self.Wk(x).reshape(B, N, self.num_heads, self.dk).permute(0, 2, 1, 3)\n",
    "        V = self.Wv(x).reshape(B, N, self.num_heads, self.dk).permute(0, 2, 1, 3)\n",
    "        \n",
    "        scores = (Q @ K.transpose(-2, -1)) * self.scale\n",
    "        attn = scores.softmax(dim=-1)\n",
    "        \n",
    "        x = (attn @ V).transpose(1, 2).reshape(B, N, C)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, 2*d_model)  \n",
    "        self.fc2 = nn.Linear(2*d_model, d_model)  \n",
    "        self.fc1_drop = nn.Dropout(dropout)\n",
    "        self.fc2_drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc1_drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2_drop(x)\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, d_model=42, num_heads=6, num_layers=4):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        self.Attention = AttentionBlock(d_model, num_heads, dropout=0.1) \n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.mlp = MLP(d_model, dropout=0.1)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        add1 = x + self.norm(self.Attention(x))\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class FullNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullNet, self).__init__()\n",
    "\n",
    "        self.embedding = Embedding()\n",
    "        self.ViT = ViT()\n",
    "\n",
    "        self.fc1 = nn.Linear(42*128, 256)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "\n",
    "        self.left1 = nn.Linear(256, 25)\n",
    "        self.left2 = nn.Linear(25, 25)\n",
    "        self.outleft = nn.Linear(25, 1)\n",
    "\n",
    "        self.right1 = nn.Linear(256, 25)\n",
    "        self.right2 = nn.Linear(25, 25)\n",
    "        self.outright = nn.Linear(25, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        print('x:',x.shape)\n",
    "\n",
    "        for i in range(4):\n",
    "            x = self.ViT(x)\n",
    "            \n",
    "        print(f'ViT-{i} out:',x.shape)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        print(f'flatten-{i} out:',x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        left = self.left1(x)\n",
    "        left = self.dropout(left)\n",
    "        left = self.left2(left)\n",
    "\n",
    "        right = self.right1(x)\n",
    "        right = self.dropout(right)\n",
    "        right = self.right2(right)\n",
    "\n",
    "        outleft = self.outleft(left)\n",
    "        outright = self.outright(right)\n",
    "\n",
    "        return {outleft, outright}\n",
    "\n",
    "\n",
    "# 输入张量 (N=1, C_in=4, D=31, H=10, W=3)\n",
    "x = torch.randn(1, 4, 31, 10, 3)    \n",
    "\n",
    "\n",
    "model = FullNet()\n",
    "y = model(x)\n",
    "print(y)\n",
    "\n",
    "# gpt修改点总结：\n",
    "# 1. Embedding: pos_emb 维度应为 (1,42,128)，并在 flatten 后加 transpose(1,2)，得到 (B,42,128)。\n",
    "# 2. ViT Block: d_model 改为 128（不是 42）；使用 Pre-LN 残差 (x = x + Attn(LN(x)))；MLP 结构为 128→256→128。\n",
    "# 3. ViT 堆叠: 共 4 层，不共享权重。\n",
    "# 4. 输出: 最终返回时用 torch.cat([...], dim=-1) 拼接成 (B,2)，不能用 set。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08beb700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.333333333333332"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45d978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
